{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7f91f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.15s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "# import kagglehub\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "# import cv2\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "GEMMA_PATH = \"models/gemma-3n-transformers-gemma-3n-e2b-it-v2\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(GEMMA_PATH)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    GEMMA_PATH,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ddbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_path = \"sample-images/images.webp\"\n",
    "analysis_prompt = \"What do you see in the image? Provide a detailed description of the objects, their colors, and any notable features.\"\n",
    "\n",
    "\n",
    "with open(image_file_path, \"rb\") as img_file:\n",
    "    image_base64_string = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_base64_string},\n",
    "            {\"type\": \"text\", \"text\": analysis_prompt}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d5fcc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation failed with error: number of heads in query/key/value should match\n",
      "\n",
      "Trying with autocast...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nipasuma/Projects/gemma-3n-challenge/venv/lib/python3.9/site-packages/torch/amp/autocast_mode.py:283: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation failed with error: number of heads in query/key/value should match\n",
      "\n",
      "Trying with autocast...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nipasuma/Projects/gemma-3n-challenge/venv/lib/python3.9/site-packages/torch/amp/autocast_mode.py:283: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation with autocast also failed: number of heads in query/key/value should match\n",
      "\n",
      "Trying manual forward pass...\n",
      "Manual forward pass failed: number of heads in query/key/value should match\n",
      "The issue appears to be deep in the model architecture.\n"
     ]
    }
   ],
   "source": [
    "# Process with Gemma 3n\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device, dtype=model.dtype)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    "    disable_compile=True,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "analysis_text = processor.batch_decode(\n",
    "    outputs[:, input_len:],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed9425e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The capital of India is **New Delhi**. \\n\\nIt's a very significant city, both historically and politically. \\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
